{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN training for crystallite size prediction\n",
    "A. Boulle and A. Souesme, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:47:41.314406Z",
     "start_time": "2024-05-23T14:47:41.302957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilities import NormalizeL, Log10Layer\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Begin user input ========\n",
    "# Path to input data\n",
    "data_path = \"/PATH/TO/DATA/FOLDER\"\n",
    "# Path to output data\n",
    "save_path = \"./checkpoints/\"\n",
    "# Define phases and and selected labels\n",
    "list_phases = [\"m_ZrO2\"]\n",
    "list_labels = [\"size\"]\n",
    "# Index corresponding to \"size\" in train_label\n",
    "indices_labels = [20]\n",
    "# Angular range used when creating the dataset. For display purpose only\n",
    "tth = np.arange(15, 120.01, 0.01)\n",
    "# Train/validation split\n",
    "validation_split = 0.1\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "# ======== End user input ========\n",
    "\n",
    "### 01. Create output folder if it does not exist\n",
    "try:\n",
    "    os.mkdir(save_path)\n",
    "    print(\"Created output folder:\", save_path)\n",
    "except:\n",
    "    print(\"Folder already exists:\", save_path)\n",
    "    pass\n",
    "\n",
    "### 02. Load training data\n",
    "print(\"Loading and formating data, please wait...\")\n",
    "raw_train = np.load(\n",
    "    data_path + \"/train.npz\", mmap_mode=\"r\"\n",
    ")  # memory map to avoid huge load\n",
    "sampling = 1  # can be used to reduce the dataset size for testing purpose\n",
    "train_data = raw_train[\"train_data\"][::sampling, :]\n",
    "train_label = raw_train[\"train_label\"][::sampling, indices_labels]\n",
    "copy_for_plot_label = np.copy(train_label)  # For display purpose only\n",
    "\n",
    "### 03. Prepare data and labels for training\n",
    "# Add empty dimension and channel\n",
    "train_data = train_data[:, :, np.newaxis, np.newaxis]\n",
    "\n",
    "# Normalize labels\n",
    "normL = NormalizeL(train_label, norm=\"MinMax\")\n",
    "train_label = normL.forward(train_label)\n",
    "\n",
    "# Save normalization class. Important to be able to convert back to real values after prediction.\n",
    "with open(save_path + \"/\" + \"normL.class\", \"wb\") as f:\n",
    "    pickle.dump(normL, f)\n",
    "\n",
    "# Shuffle indices\n",
    "nb_data = train_data.shape[0]\n",
    "indices = np.arange(nb_data)\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_label = train_label[indices]\n",
    "copy_for_plot_label = copy_for_plot_label[indices]  # For display purpose only\n",
    "\n",
    "# Get shapes\n",
    "input_shape = train_data[0].shape\n",
    "label_shape = train_label[0].shape\n",
    "\n",
    "\n",
    "### 04. Instead of loading the full dataset in memory, we use the TF Dataset format and a generator\n",
    "# This prevents memory issues when using large datasets.\n",
    "# Generator function\n",
    "def data_generator():\n",
    "    for x, y in zip(train_data, train_label):\n",
    "        yield x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "# Dataset definition\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=label_shape, dtype=tf.float32),\n",
    ")\n",
    "full_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator, output_signature=output_signature\n",
    ")\n",
    "\n",
    "nb_val = int(nb_data * validation_split)\n",
    "\n",
    "train_dataset = full_dataset.skip(nb_val).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = full_dataset.take(nb_val).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Done. Dataset ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multiple times to see different patterns\n",
    "random_data = np.random.randint(nb_data)\n",
    "print(f\"size = {copy_for_plot_label[random_data][0]:.3f} Âµm\")\n",
    "plt.cla()\n",
    "plt.semilogy(tth, train_data[random_data, :, 0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer with two parallel convolutions\n",
    "def conv_block(x, filters, strideC1=(2, 1), strideC2=(1, 1)):\n",
    "    shortcut = x\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters, (filter_length, 1), strides=strideC1, padding=\"same\"\n",
    "    )(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters, (filter_length, 1), strides=strideC2, padding=\"same\"\n",
    "    )(x)\n",
    "    if strideC2 != (1, 1) or shortcut.shape[-1] != filters:\n",
    "        shortcut = keras.layers.Conv2D(\n",
    "            filters, (filter_length, 1), strides=strideC1, padding=\"same\"\n",
    "        )(shortcut)\n",
    "    x = keras.layers.Add()([x, shortcut])\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN architecture\n",
    "filter_length = 10\n",
    "pool_length = 2\n",
    "activation = \"relu\"\n",
    "use_bias = True\n",
    "kernels_per_layer = [32, 64, 128, 256, 512]\n",
    "# Number of output neurons\n",
    "nbr_output_neurons = len(list_phases) * len(list_labels)\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "x = Log10Layer()(inputs)  # Log10 normalization\n",
    "# x = MinMaxScalingLayer(axis=1)(x)  # Min-Max scaling\n",
    "x = conv_block(x, 16)\n",
    "x = keras.layers.MaxPool2D((pool_length, 1))(x)\n",
    "\n",
    "for kernels in kernels_per_layer:\n",
    "    x = conv_block(x, kernels)\n",
    "    x = keras.layers.MaxPool2D((pool_length, 1))(x)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(1000, activation=activation)(x)\n",
    "\n",
    "outputs = keras.layers.Dense(nbr_output_neurons)(x)\n",
    "\n",
    "cnn = keras.Model(inputs=inputs, outputs=outputs, name=\"CNN\")\n",
    "cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss=\"mse\")\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks used for training: reduce LR on plateau and save best model\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=save_path + \"/CNN.hdf5\",\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"auto\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Training. Adjust number of epochs as needed.\n",
    "epochs = 5\n",
    "\n",
    "cnn_hist = cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.semilogy(cnn_hist.history[\"loss\"], label=\"Training Loss\")\n",
    "ax1.semilogy(cnn_hist.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.semilogy(cnn_hist.history[\"lr\"], \":r\")\n",
    "ax2.set_ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
